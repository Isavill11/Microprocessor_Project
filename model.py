
import os
import numpy as np
import pandas as pd

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns



CSV_PATH = "updated_air_quality_dataset.csv"

# If None, the script tries to auto-detect among these names:
LABEL_COL = 'pollution_level'
LABEL_CANDIDATES = ["pollution_level", "class", "label", "y"]

# Decision tree size constraints (small = easier/faster on 8051)
MAX_DEPTH = 6          # keep the tree shallow
MIN_SAMPLES_LEAF = 8   # avoid tiny leaves
RANDOM_STATE = 42      # reproducibility

# Whether to export per-feature scaling stats (min, max) alongside the model.
# Useful if you want to quantize on-device; otherwise, you can quantize off-device.
EXPORT_SCALER_LIMITS = True

# Output header filename for Keil
HEADER_OUT = "dt_model_2.h"


def infer_label_column(df: pd.DataFrame) -> str:
    for name in LABEL_CANDIDATES:
        if name in df.columns:
            return name
    raise ValueError(
        f"Could not find a label column. Set LABEL_COL or rename one of: {LABEL_CANDIDATES}"
    )

def split_X_y(df: pd.DataFrame, label_col: str):

    y = df[label_col].astype(str)  # treat labels as strings/classes
    X = df.drop(columns=[label_col])

    # Ensure all features are numeric
    non_num = [c for c in X.columns if not np.issubdtype(X[c].dtype, np.number)]
    if non_num:
        raise TypeError(
            f"Found non-numeric feature columns: {non_num}. Convert them to numbers before training."
        )

    return X, y

def quantize_to_uint8(X: np.ndarray):    
    mins = X.min(axis=0)
    maxs = X.max(axis=0)
    denom = np.where((maxs - mins) == 0, 1.0, (maxs - mins))
    X_norm = (X - mins) / denom
    X_q = np.round(X_norm * 255.0).astype(np.uint8)
    return X_q, mins, maxs

def export_tree_as_c(clf: DecisionTreeClassifier,
                     out_path: str,
                     feature_mins=None,
                     feature_maxs=None):
        
    t = clf.tree_
    n_nodes = t.node_count

    # Feature indices; sklearn uses -2 for leaves in t.feature? No; sklearn uses -2 in t.children_* to mark leaf.
    # Leaf detection: if children_left[i] == children_right[i] == -1, then it's a leaf.
    feature = t.feature.copy()  # shape (n_nodes,), int
    threshold_float = t.threshold.copy()  # float thresholds

    # Build arrays with consistent leaf marker:
    LEAF_MARK = -2
    tree_feature = []
    tree_threshold = []
    tree_left = []
    tree_right = []
    tree_value = []  # class-1 probability at node

    # For binary classification, t.value shape is (n_nodes, 1, 2)
    # We will derive P(class==1) for both decision and leaves.
    for i in range(n_nodes):
        left_i = t.children_left[i]
        right_i = t.children_right[i]

        # Value vector at node; sum across classes
        counts = t.value[i, 0, :]   # (n_classes,) counts at node
        total = np.sum(counts)
        p1 = float(counts[1] / total) if total > 0 else 0.0

        if left_i == -1 and right_i == -1:
            # Leaf node
            tree_feature.append(LEAF_MARK)
            tree_threshold.append(0)  # unused
            tree_left.append(-1)
            tree_right.append(-1)
            tree_value.append(p1)
        else:
            # Decision node
            tree_feature.append(int(feature[i]))
            # Quantized threshold: round to nearest integer in [0..255]
            th = int(np.clip(np.round(threshold_float[i]), 0, 255))
            tree_threshold.append(th)
            tree_left.append(int(left_i))
            tree_right.append(int(right_i))
            tree_value.append(p1)

    # Write header
    with open(out_path, "w") as f:
        f.write("// Auto-generated decision tree header for 8051 (Keil C)\n")
        f.write("// Generated by gas_drift_to_8051.py\n\n")
        f.write("#ifndef DT_MODEL_H\n#define DT_MODEL_H\n\n")
        f.write("#include <stdint.h>\n\n")

        # Macros
        f.write("// Node index of root\n#define TREE_ROOT (0)\n")
        f.write(f"// Total nodes\n#define TREE_SIZE ({n_nodes})\n")
        f.write("// Leaf marker in tree_feature[]\n#define TREE_LEAF (-2)\n\n")

        # Arrays
        def write_array_int16(name, arr):
            f.write(f"static const int16_t {name}[TREE_SIZE] = {{\n  ")
            f.write(", ".join(str(int(v)) for v in arr))
            f.write("\n};\n\n")

        def write_array_uint8(name, arr):
            f.write(f"static const uint8_t {name}[TREE_SIZE] = {{\n  ")
            f.write(", ".join(str(int(v)) for v in arr))
            f.write("\n};\n\n")

        def write_array_float(name, arr):
            f.write(f"static const float {name}[TREE_SIZE] = {{\n  ")
            f.write(", ".join(f"{float(v):.6f}" for v in arr))
            f.write("\n};\n\n")

        write_array_int16("tree_feature", tree_feature)
        write_array_uint8("tree_threshold", tree_threshold)
        write_array_int16("tree_children_left", tree_left)
        write_array_int16("tree_children_right", tree_right)
        write_array_float("tree_values", tree_value)

        # Optional: export scaler mins/maxs so MCU can quantize raw ADC features the same way
        if EXPORT_SCALER_LIMITS and (feature_mins is not None) and (feature_maxs is not None):
            nfeat = len(feature_mins)
            f.write(f"// Per-feature min/max used for 0..255 quantization\n")
            f.write(f"#define N_FEATURES ({nfeat})\n")
            f.write(f"static const float feature_min[N_FEATURES] = " +
                    "{ " + ", ".join(f"{float(v):.6f}" for v in feature_mins) + " };\n")
            f.write(f"static const float feature_max[N_FEATURES] = " +
                    "{ " + ", ".join(f"{float(v):.6f}" for v in feature_maxs) + " };\n\n")
        else:
            # If not exporting, at least define N_FEATURES for array sizes in C examples
            nfeat = clf.n_features_in_
            f.write(f"#define N_FEATURES ({nfeat})\n\n")

        f.write("#endif // DT_MODEL_H\n")
    print(f"[OK] Wrote model header to: {out_path}")

def plot_cm(cm, y_test):
        # Get the unique class labels for the axes
        class_labels = sorted(y_test.unique()) 
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(
            cm, 
            annot=True,              # Annotate cells with the numeric value
            fmt='d',                 # Format as an integer (decimal)
            cmap='Blues',            # Color map
            xticklabels=class_labels, 
            yticklabels=class_labels
        )
        plt.title('Confusion Matrix on Quantized Test Set')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.show() # Display the plot



def main():
    # 1) Check that CSV exists
    if not os.path.exists(CSV_PATH):
        raise FileNotFoundError(
            f"CSV not found at: {CSV_PATH}\n"
            "Place a merged Gas Sensor Array Drift CSV there.\n"
            "Dataset info: https://archive.ics.uci.edu/ml/datasets/gas+sensor+array+drift+dataset"
        )

    # 2) Load CSV
    df = pd.read_csv(CSV_PATH)
    df.dropna(axis=1)
    cols_to_drop = ['SITE_ID', 'DATEOFF', 'DATEON']
    df = df.drop(columns=cols_to_drop, errors='ignore')
    print(f"[PREPROC] Dropped non-feature columns: {cols_to_drop}")

    label_col = LABEL_COL if LABEL_COL else infer_label_column(df)


    # --- Strategy 2: Convert columns that should be numeric ---
    # The remaining columns (Ca, Cl, etc.) appear to be measurements.
    # We will force them to be numeric, setting any values that can't be converted (like 'NaN' strings) to actual NaN.
    numeric_candidates = ['Ca', 'Cl', 'HNO3 PPB', 'K', 'Mg', 'Na', 'NH4', 'SO2 PPB', 'TNO3']
    for col in numeric_candidates:
        if col in df.columns:
            # 'coerce' will turn non-numeric strings into NaN (which SimpleImputer handles)
            df[col] = pd.to_numeric(df[col], errors='coerce')
            
    print("[PREPROC] Converted measurement columns to numeric (coercing errors to NaN).")
    
    # 5) Split features and labels
    X_df, y = split_X_y(df, label_col)

    # 6) Impute missing values (mean) - Original Step 5
    imp = SimpleImputer(strategy="mean")
    X_imp = imp.fit_transform(X_df.values)


    # 6) Quantize features to 0..255 (uint8); also keep mins/maxs for possible MCU quantization
    X_q, mins, maxs = quantize_to_uint8(X_imp)

    # 7) Train/test split
    X_train, X_test, y_train, y_test = train_test_split(
        X_q, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y
    )

    # 8) Train a tiny Decision Tree (shallow)
    clf = DecisionTreeClassifier(
        max_depth=MAX_DEPTH,
        min_samples_leaf=MIN_SAMPLES_LEAF,
        random_state=RANDOM_STATE
    )
    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_test)
    print("\n=== Evaluation on quantized test set ===")
    print(classification_report(y_test, y_pred))
    
    # Calculate the confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    print("Confusion matrix:\n", cm)

    plot_cm(cm, y_test)    


    # 11) Export the tree to a C header
    export_tree_as_c(
        clf,
        out_path=HEADER_OUT,
        feature_mins=mins if EXPORT_SCALER_LIMITS else None,
        feature_maxs=maxs if EXPORT_SCALER_LIMITS else None
    )

    print("\n[Done] You can now include dt_model.h in Keil and run the C inference.")

if __name__ == "__main__":
    main()
